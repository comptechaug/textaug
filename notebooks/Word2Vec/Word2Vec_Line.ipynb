{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec augmentations line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузить данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv('../../data/twitts/positive.csv', sep=';', header=None).assign(positive=1)\n",
    "df_neg = pd.read_csv('../../data/twitts/negative.csv', sep=';', header=None).assign(positive=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_pos[[3,'positive']].append( df_neg[[3,'positive']], ignore_index=True ).rename({3:'tweet'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ну любишь или нет? — Я не знаю кто ты бля:D ht...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @SpoonLamer: Ох,900 :D ну это конечно же @t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @veregijytaqo: У тебя есть ухажёр? Нет - мо...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Поприветствуем моего нового читателя @Alexey17...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Теперь у меня есть частичка Сиднея :) #Sydney ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Люблю маму и папу!!!!а в остальное я так...-вл...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT @dicyziqecida: Как-то я забыла, что вчера п...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@MrsRourke_s_tit @_vivante @dyu_bryun так было...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@Kruglova_Julia_  дааааа))\\nТы... Ты... Ты и т...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@Alex_Shvarz :)) О, нет. Вы ведь всё равно обз...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  positive\n",
       "0   @first_timee хоть я и школота, но поверь, у на...         1\n",
       "1   Да, все-таки он немного похож на него. Но мой ...         1\n",
       "2   RT @KatiaCheh: Ну ты идиотка) я испугалась за ...         1\n",
       "3   RT @digger2912: \"Кто то в углу сидит и погибае...         1\n",
       "4   @irina_dyshkant Вот что значит страшилка :D\\nН...         1\n",
       "5   ну любишь или нет? — Я не знаю кто ты бля:D ht...         1\n",
       "6   RT @SpoonLamer: Ох,900 :D ну это конечно же @t...         1\n",
       "7   RT @veregijytaqo: У тебя есть ухажёр? Нет - мо...         1\n",
       "8   Поприветствуем моего нового читателя @Alexey17...         1\n",
       "9   Теперь у меня есть частичка Сиднея :) #Sydney ...         1\n",
       "10  Люблю маму и папу!!!!а в остальное я так...-вл...         1\n",
       "11  RT @dicyziqecida: Как-то я забыла, что вчера п...         1\n",
       "12  @MrsRourke_s_tit @_vivante @dyu_bryun так было...         1\n",
       "13  @Kruglova_Julia_  дааааа))\\nТы... Ты... Ты и т...         1\n",
       "14  @Alex_Shvarz :)) О, нет. Вы ведь всё равно обз...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop_duplicates() #важно, потому что дубликаты кажется есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    110396\n",
       "0    107044\n",
       "Name: positive, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.positive.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убрать из данных характерные для твиттера вещи (RT, @ и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tweet = tweets['tweet'].apply(lambda x: re.sub('[\\Wa-zA-Z_\\d]+', ' ', x) )\n",
    "tweets.tweet = tweets['tweet'].apply(lambda x: x.lower().strip() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного пристальнее посмотреть на записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 а стояк теперь снова стояк вселенная вернула все на свои места\n",
      "2 прифетигг солныско каг тфаи дила мну сичас фф больниську идеть к логопеду\n",
      "3 да нашла фоточку в иг и нагло спиздила ничего с фотографиями меня уже статусы в вконтакте бывают\n",
      "4 а у нас в нижнем слякотная погода брр но у нас офигенный привоз заходим заходим\n",
      "5 короче видимо ни в какой архитектурный я не иду там капец конечно\n",
      "6 ну в сделаешь тогда я дарья а ты\n",
      "7 плак плак надеюсь на драфте тогда их вытяну\n",
      "8 а моя кроме поорать еще просится на балкон хвост поморозить минуту посидит и назад просится\n",
      "9 каждому так болеть когда к тебе друзья в гости ходят\n",
      "10 мам мне так хреново выпей таблетку и на работу на следующей недели пойдем в сауну мама умеет поддержать и добавить стимула\n"
     ]
    }
   ],
   "source": [
    "for i, v in zip(range(1,100), tweets['tweet'][50:60]):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прелестно.\n",
    "Пора переводить данные, но сначала разделю на train и test части, чтоб аугментировать только train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets.tweet\n",
    "y = tweets.positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163080,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V augmentation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачать файл с векторами [отсюда](http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz100-w10-cb0-it1-min100.w2v)\n",
    "\n",
    "Я использую самый лёгкий файл, но в перспективе можно и более тяжёлые, всё лежит [здесь](https://nlpub.mipt.ru/Russian_Distributional_Thesaurus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mipt_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "     '/home/nikitap/projects/python/augmentations_comptech/Word2Vec/mipt_vecs.w2v', binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишу тупенькую функцию, чтоб аугментировать предложения, но сначала ещё кое-что"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mipt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неисследованная опция `num_trees` кажется улучшает качество поиска похожего слова; если компьютер мощный, то можно попробовать поднять значение до `100-500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitap/programs/anaconda3/lib/python3.7/site-packages/gensim/similarities/index.py:184: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  index = AnnoyIndex(num_features)\n"
     ]
    }
   ],
   "source": [
    "from gensim.similarities.index import AnnoyIndexer #вопрос, может ли работать\n",
    "annoy_index = AnnoyIndexer(mipt_model, num_trees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартный поиск наиболее похожего слова слишком долгий, поэтому буду пользоваться [`AnnoyIndexer`](https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 µs ± 33.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "93.5 ms ± 4.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model.most_similar('слово', topn=2, indexer=annoy_index)\n",
    "%timeit model.most_similar('слово', topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy: [('слово', 1.0), ('произнесено', 0.6672296822071075)]\n",
      "Standard: [('слово…', 0.8392893671989441), ('словечко', 0.794983983039856)]\n"
     ]
    }
   ],
   "source": [
    "print( 'Annoy:', model.most_similar('слово', topn=2, indexer=annoy_index) )\n",
    "print( 'Standard:', model.most_similar('слово', topn=2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sen_aug_one_change(sen):\n",
    "    words = sen.split(' ')\n",
    "    indexes = list( range(len(words)) )\n",
    "    random.shuffle(indexes)\n",
    "    new_words = list()\n",
    "    for i in indexes:\n",
    "        if words[i] in model:\n",
    "            words[i] = model.most_similar(words[i], topn=1)[0][0]\n",
    "            return ' '.join(words)\n",
    "    return sen  # no replacement was possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sen_aug_one_change_annoy(sen):\n",
    "    words = sen.split(' ')\n",
    "    indexes = list( range(len(words)) )\n",
    "    random.shuffle(indexes)\n",
    "    new_words = list()\n",
    "    for i in indexes:\n",
    "        if words[i] in model:\n",
    "            words[i] = model.most_similar(words[i], topn=2, indexer=annoy_index)[1][0]\n",
    "            return ' '.join(words)\n",
    "    return sen  # no replacement was possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sen_aug_all_changes(sen):\n",
    "    words = sen.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in model:\n",
    "            words[i] = model.most_similar(words[i], topn=3)[1][0]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sen_aug_all_changes_annoy(sen):\n",
    "    words = sen.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in model:\n",
    "            words[i] = model.most_similar(words[i], topn=3, indexer=annoy_index)[1][0]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример полностью переделанных твитов, но для быстроты будем менять по одному слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: но я не обронила этих слов это что привычка или что я не поиму\n",
      "All change words: не мне так промолвила таких выражений что не привычка — например не мне так поиму\n",
      "All change words (with annoy): ли меня подумать усмехнулась других фраз запутали, — действительно забывчивость соответствующего действительно меня подумать поиму\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(tweets))\n",
    "print('Original tweet:', tweets.tweet[i])\n",
    "print('All change words:', sen_aug_all_changes(tweets.tweet[i]) )\n",
    "print('All change words (with annoy):', sen_aug_all_changes_annoy(tweets.tweet[i]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравниваю функции по скорости работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47 s ± 66.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "4.64 ms ± 635 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "98.4 ms ± 5.46 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "416 µs ± 18.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sen_aug_all_changes(X_w2v[0])\n",
    "%timeit sen_aug_all_changes_annoy(X_w2v[0])\n",
    "%timeit sen_aug_one_change(X_w2v[0])\n",
    "%timeit sen_aug_one_change_annoy(X_w2v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С учётом количества данных в `X_w2v` = 163080, предпочитаю скорость качеству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.5 s, sys: 4.97 s, total: 51.5 s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%time X_add = X_w2v.apply(lambda x: sentence2sentence_shuffle(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры того, какие аугментации получились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin: забрал у меня айфон и смайлики айфоновские шлешь\n",
      "Aug: яварски у меня айфон и смайлики айфоновские шлешь\n",
      "__________\n",
      "Origin: нелегкое детство будущего дизайнера\n",
      "Aug: нелегкое детство перспективы дизайнера\n",
      "__________\n",
      "Origin: сейчас я могла бы смотреть тетрадь смерти и лопать вкусняшки но сижу под дверью и жду родителей\n",
      "Aug: сейчас я могла бы смотреть тетрадь смерти и лопать вкусняшки но сижу под двери и жду родителей\n",
      "__________\n",
      "Origin: цагаан хоолтон гэж амьтны амь таслахыг тэвчсэн нөхдүүд байдаг уу\n",
      "Aug: мангасов хоолтон гэж амьтны амь таслахыг тэвчсэн нөхдүүд байдаг уу\n",
      "__________\n",
      "Origin: мне урупон шею свернет лента я вас любил с\n",
      "Aug: мне урупон шею помчится лента я вас любил с\n",
      "__________\n",
      "Origin: ахахахахахах нет это слишком пошло\n",
      "Aug: ахахахахахах нет это слишком поехало…\n",
      "__________\n",
      "Origin: лал мне уже интересно что это за сюрприз с\n",
      "Aug: лал мне уже интересно что это уговорив сюрприз с\n",
      "__________\n",
      "Origin: я так не хотела в школу и хотела ливень с громом и молнией что пошел ливень с громом и молнией\n",
      "Aug: я так не хотела в школу йотом хотела ливень с громом и молнией что пошел ливень с громом и молнией\n",
      "__________\n",
      "Origin: еду в метро в час пик\n",
      "Aug: еду в метро первой час пик\n",
      "__________\n",
      "Origin: ария была худее и красивее а сейчас ужас верните мне прежнюю арию\n",
      "Aug: ария была худее и красивее а сейчас ужаса верните мне прежнюю арию\n",
      "__________\n",
      "Origin: холи щит не думал что выйграю ключ словами не передать восторг огромное спасибо теперь вовсе забью на учебу\n",
      "Aug: холи щит не думал что выйграю ключ словами не передать восторг огромное спасибо нуждались… вовсе забью на учебу\n",
      "__________\n",
      "Origin: статистика ливинтернет принебрегает хипстерней\n",
      "Aug: статистике ливинтернет принебрегает хипстерней\n",
      "__________\n",
      "Origin: слишком но у меня все слишком даже болею слишком\n",
      "Aug: слишком но у меня все слишком уверяли болею слишком\n",
      "__________\n",
      "Origin: скорее не заплатил а не дополучил плюс это долги со времен ссср\n",
      "Aug: скорее не заплатил а не дополучил плюс это долги со сохранявшихся ссср\n",
      "__________\n",
      "Origin: я снова в режиме ничего не успеваю и безумно хочу спать\n",
      "Aug: я снова первой режиме ничего не успеваю и безумно хочу спать\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "n_tweets = 15\n",
    "ind = random.randint(n_tweets, len(X_w2v))\n",
    "for w2v,add in zip(X_w2v.values[ind-15:ind], X_add.values[ind-n_tweets:ind]):\n",
    "    print('Origin:', w2v)\n",
    "    print('Aug:', add )\n",
    "    print('__________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_w2v.append( X_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.append(y_train, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326160 326160\n"
     ]
    }
   ],
   "source": [
    "print( X_train.shape[0], y_train.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова нужно вычистить дупликаты, сделаю это немного странно, т.к. нужно деражать правильные ответы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'tweet': X_train.apply(lambda x: re.sub('(\\W+|\\d+|[A-Za-z]+|_+)', ' ', x) ), \n",
    "                         'positive': y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>пишу реферат без копипаст чертов антиплагиат ч...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ночнойминск и я спать всем спокойной ночи и сл...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>снова попал в аварию и снова все живы блеать</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>как обычно ляпну не подумавши</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>я так устал а завтра еще и разгрузка за то под...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326155</th>\n",
       "      <td>сегодня суббота а я весь день учу этот бух уче...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326156</th>\n",
       "      <td>должна отметить что новый клип майли сайрус по...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326157</th>\n",
       "      <td>я тоже на приветливого фермера и игр типа таких</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326158</th>\n",
       "      <td>капюшон моей куртки с синим оттенком рекламка ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326159</th>\n",
       "      <td>жители академгородка получат скидку на оптику ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>326160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  positive\n",
       "0       пишу реферат без копипаст чертов антиплагиат ч...         0\n",
       "1       ночнойминск и я спать всем спокойной ночи и сл...         1\n",
       "2            снова попал в аварию и снова все живы блеать         0\n",
       "3                           как обычно ляпну не подумавши         0\n",
       "4       я так устал а завтра еще и разгрузка за то под...         0\n",
       "...                                                   ...       ...\n",
       "326155  сегодня суббота а я весь день учу этот бух уче...         0\n",
       "326156  должна отметить что новый клип майли сайрус по...         0\n",
       "326157    я тоже на приветливого фермера и игр типа таких         0\n",
       "326158  капюшон моей куртки с синим оттенком рекламка ...         1\n",
       "326159  жители академгородка получат скидку на оптику ...         0\n",
       "\n",
       "[326160 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318322"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.tweet, df_train.positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На всякий случай можно ещё раз почистить, ещё раз убрать все знаки препинания и цифры\n",
    "\n",
    "Но для начала нужно слить train и test, чтоб vectorize все строки, поэтому запоминаю `split_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = X_train.shape[0]\n",
    "X = X_train.append(X_test).apply(lambda x: re.sub('(\\W+|\\d+|[A-Za-z]+|_+)', ' ', x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326150    читай зайдешь на страницу работавшего настроен...\n",
       "326151    или вот занусси да и вообще любой поляк а фран...\n",
       "326152    россия московская область г егорьевск микр н с...\n",
       "326153    лида разочаровала нас печальной новостью почти...\n",
       "326154    думала зайду на минут поиграю в симс первой ит...\n",
       "326155    сегодня суббота а я весь день учу этот бух уче...\n",
       "326156    должна отметить что новый клип майли сайрус по...\n",
       "326157      я тоже на приветливого фермера и игр типа таких\n",
       "326158    капюшон моей куртки с синим оттенком рекламка ...\n",
       "326159    жители академгородка получат скидку на оптику ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[split_index-10:split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_test) + len(X_train) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372682"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почищено, теперь векторизую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vect = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова разделяю по split индексу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect_train, X_vect_test = X_vect[:split_index,], X_vect[split_index:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры верны: True\n",
      "Размеры верны: True\n"
     ]
    }
   ],
   "source": [
    "print('Размеры верны:', X_vect_train.shape[0] == y_train.shape[0] )\n",
    "print('Размеры верны:', X_vect_test.shape[0] == y_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['автоматы',\n",
       " 'автомеханика',\n",
       " 'автомобиле',\n",
       " 'автомобилей',\n",
       " 'автомобилем',\n",
       " 'автомобили',\n",
       " 'автомобилист',\n",
       " 'автомобилистов',\n",
       " 'автомобилисты',\n",
       " 'автомобиль',\n",
       " 'автомобильной',\n",
       " 'автомобильном',\n",
       " 'автомобильные',\n",
       " 'автомобильный',\n",
       " 'автомобильчик',\n",
       " 'автомобиля',\n",
       " 'автомойка',\n",
       " 'автоновости',\n",
       " 'автономии',\n",
       " 'автономия']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[800:820]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go учиться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=400,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_vect_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_vect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54360,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7336460632818249"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7452001759788827"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ок, хорошо\n",
    "\n",
    "теперь можно менять параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(clf, {'C': [0.75, 1, 1.25]}, cv=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=400, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None, param_grid={'C': [0.75, 1, 1.25]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_vect_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.25}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_vect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7334988962472406"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7446009555206883"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
