{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu\n",
    "# !pip install keras\n",
    "# !pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFxy9VfywWj0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Ilya\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "import tokenization\n",
    "import re\n",
    "\n",
    "class BertContextAugmentation():\n",
    "    #TODO try catch\n",
    "    def __init__(self, model_folder):\n",
    "        self.folder = model_folder\n",
    "        self.config_path = folder+'/bert_config.json'\n",
    "        self.checkpoint_path = folder+'/bert_model.ckpt'\n",
    "        self.vocab_path = folder+'/vocab.txt'\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file=self.vocab_path, do_lower_case=False)\n",
    "        self.model = load_trained_model_from_checkpoint(self.config_path, self.checkpoint_path, training=True)\n",
    "        \n",
    "        \n",
    "    def bert_aug(self, cls_sentence):\n",
    "        # предсказание слов, закрытых токеном MASK в фразе. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в [MASK] и купил [MASK]. [SEP]\n",
    "\n",
    "        # входная фраза с закрытыми словами с помощью [MASK]\n",
    "        #sentence = 'Я пришел в [MASK] и купил [MASK].'  #@param {type:\"string\"}\n",
    "        out_sentence = cls_sentence\n",
    "        sentence = cls_sentence\n",
    "\n",
    "        # преобразование в токены (tokenizer.tokenize() не обрабатывает [CLS], [MASK], поэтому добавим их вручную)\n",
    "        sentence = sentence.replace(' [MASK] ','[MASK]'); sentence = sentence.replace('[MASK] ','[MASK]'); sentence = sentence.replace(' [MASK]','[MASK]')  # удаляем лишние пробелы. Можно заменить регуляркой \"\\s?\\[MASK\\]\\s?\", но это надо импортить re\n",
    "        sentence = sentence.split('[MASK]')             # разбиваем строку по маске\n",
    "        tokens = ['[CLS]']                              # фраза всегда должна начинаться на [CLS]\n",
    "        # обычные строки преобразуем в токены с помощью tokenizer.tokenize(), вставляя между ними [MASK]\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                tokens = tokens + self.tokenizer.tokenize(sentence[i]) \n",
    "            else:\n",
    "                tokens = tokens + ['[MASK]'] + self.tokenizer.tokenize(sentence[i]) \n",
    "        tokens = tokens + ['[SEP]']                     # фраза всегда должна заканчиваться на [SEP] \n",
    "        # в tokens теперь токены, которые гарантированно по словарю преобразуются в индексы\n",
    "        \n",
    "        # преобразуем в массив индексов, который можно подавать на вход сети, причем число 103 в нем это [MASK]\n",
    "        token_input = self.tokenizer.convert_tokens_to_ids(tokens)        \n",
    "        # удлиняем до 512 длины\n",
    "        token_input = token_input + [0] * (512 - len(token_input))\n",
    "        \n",
    "        \n",
    "        # создаем маску, заменив все числа 103 на 1, а остальное 0\n",
    "        mask_input = [0]*512\n",
    "        for i in range(len(mask_input)):\n",
    "            if token_input[i] == 103:\n",
    "                mask_input[i] = 1\n",
    "        #print(mask_input)\n",
    "        \n",
    "        # маска фраз (вторая фраза маскируется числом 1, а все остальное числом 0)\n",
    "        seg_input = [0]*512\n",
    "        \n",
    "        \n",
    "        # конвертируем в numpy в форму (1,) -> (1,512)\n",
    "        token_input = np.asarray([token_input])\n",
    "        mask_input = np.asarray([mask_input])\n",
    "        seg_input = np.asarray([seg_input])\n",
    "        \n",
    "        \n",
    "        # пропускаем через нейросеть...\n",
    "        predicts = self.model.predict([token_input, seg_input, mask_input])[0]       # в [0] полная фраза с заполненными предсказанными словами на месте [MASK]\n",
    "        predicts = np.argmax(predicts, axis=-1)\n",
    "        \n",
    "        \n",
    "        # форматируем результат в строку, разделенную пробелами\n",
    "        predicts = predicts[0][:len(tokens)]    # длиной как исходная фраза (чтобы отсечь случайные выбросы среди нулей дальше)\n",
    "        out = []\n",
    "        # добавляем в out только слова в позиции [MASK], которые маскированы цифрой 1 в mask_input\n",
    "        for i in range(len(mask_input[0])):\n",
    "            if mask_input[0][i] == 1:           # [0][i], т.к. требование было (1,512)\n",
    "                out.append(predicts[i]) \n",
    "        \n",
    "        out = self.tokenizer.convert_ids_to_tokens(out)# индексы в токены\n",
    "        \n",
    "        for i in range(len(re.findall('\\[MASK\\]?', out_sentence))):\n",
    "            out_sentence = re.sub('\\[MASK\\]', out[i], out_sentence, 1)\n",
    "\n",
    "        return out_sentence\n",
    "    \n",
    "    def choose_random_place(self, sentence, sent_length):\n",
    "        \"\"\"\n",
    "        Выбирает рандомные места в предложении куда в дальнейшем вставляет слово\n",
    "        \"\"\"\n",
    "        aug_num = np.random.randint(1, sent_length // 3 + 1)\n",
    "        splited_sent = sentence.split(' ')\n",
    "        for i in range(aug_num):\n",
    "            splited_sent = splited_sent[:]; splited_sent.insert(np.random.randint(1, sent_length),'[MASK]')\n",
    "        return ' '.join(splited_sent)\n",
    "            \n",
    "        \n",
    "    def choose_random_word(self, sentence, sent_length):\n",
    "        \"\"\"\n",
    "        Выберает рандомное слово которое будет заменено\n",
    "        \"\"\"\n",
    "        aug_num = np.random.randint(1, sent_length // 3 + 1)\n",
    "        splited_sent = sentence.split(' ')\n",
    "        for i in range(aug_num):\n",
    "            rand_ind = np.random.randint(0, sent_length)\n",
    "            for n, i in enumerate(splited_sent):\n",
    "                if n == rand_ind:\n",
    "                    splited_sent[n] = '[MASK]'\n",
    "        return ' '.join(splited_sent)    \n",
    "        \n",
    "    def make_single_aug(self, sentence, sent_length):\n",
    "        \"\"\"\n",
    "        Применяет одну из двух аугментаций к предложению\n",
    "        \"\"\"\n",
    "        if np.random.randint(0, 2) == 0:\n",
    "            aug_sentence = self.bert_aug(self.choose_random_place(sentence, sent_length))\n",
    "        else:\n",
    "            aug_sentence = self.bert_aug(self.choose_random_word(sentence, sent_length))\n",
    "        return aug_sentence\n",
    "    \n",
    "    def try_another_one_aug(self, sentence, sent_length, attempts=1, trys=3):\n",
    "        \"\"\"\n",
    "        Пытается применить аугментацию пока не получит новое предложение.\n",
    "        \"\"\"\n",
    "        if attempts <= trys:\n",
    "            aug_sentence = self.make_single_aug(sentence, sent_length)\n",
    "            if aug_sentence == sentence:\n",
    "                self.try_another_one_aug(sentence, sent_length, attempts=attempts+1, trys=trys)\n",
    "            else:\n",
    "                return aug_sentence\n",
    "        return sentence\n",
    "\n",
    "    def make_aug(self, sentence, attempts=1, trys=3):\n",
    "        sent_length = len(sentence.split(' '))\n",
    "        if sent_length // 3 > 0:\n",
    "            aug_sentence = self.try_another_one_aug(sentence, sent_length, attempts=attempts, trys=trys)\n",
    "            return aug_sentence\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ilya\\Documents\\Projects\\textaug\\notebooks\\BertContextAugmentation\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folder = 'models/ru_conversational_cased_L-12_H-768_A-12'\n",
    "bert = BertContextAugmentation(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cls_sentence = 'Я пришел в магазин и купил молоко.'\n",
    "out = bert.make_aug(cls_sentence)\n",
    "\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
